{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sqlite3\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('Harry_Potter80_filled.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "poss = ['NUM', 'X', 'ADJ', 'SYM', 'ADV', 'AUX', 'ADP', 'PROPN', 'NOUN', 'SCONJ', 'CCONJ', 'DET', 'PRON', 'PART', 'VERB', 'INTJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите запрос: \"Гарри\" Поттер VERB\n",
      "['\"Гарри\"', 'Поттер', 'VERB']\n"
     ]
    }
   ],
   "source": [
    "request = input('Введите запрос: ')\n",
    "request_items = re.split(' ', request)\n",
    "print(request_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## запрос для двух частей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part_2 = '''\n",
    "    select * FROM\n",
    "    (select texts.title, b.sentence, t.word as first, \n",
    "        (select s.word from tokens s \n",
    "        join main on s.token_id = main.token_id\n",
    "        join sentences p on p.sentence_id = main.sentence_id\n",
    "        where s.token_id = t.token_id + 1 \n",
    "        and s'''\n",
    "\n",
    "second_part_2 = ''' and b.sentence_id = p.sentence_id) second\n",
    "    from \n",
    "    tokens t\n",
    "    join main on t.token_id = main.token_id\n",
    "    join sentences b on b.sentence_id = main.sentence_id\n",
    "    join texts on b.text_id = texts.text_id\n",
    "    where t'''\n",
    "\n",
    "third_part_2 = ''')\n",
    "    where not (second is null)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## запрос для трёх частей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_part_3 = '''\n",
    "    select * FROM\n",
    "    (select t.token_id, t.word,\n",
    "        (select s.word\n",
    "        from tokens s\n",
    "        join main on s.token_id = main.token_id\n",
    "        join sentences p on p.sentence_id = main.sentence_id\n",
    "        where s.token_id = t.token_id + 1\n",
    "        and s'''\n",
    "\n",
    "second_part_3 = ''' and b.sentence_id = p.sentence_id) second,\n",
    "        (select s.word from tokens s\n",
    "        join main on s.token_id = main.token_id\n",
    "        join sentences p on p.sentence_id = main.sentence_id\n",
    "        where s.token_id = t.token_id + 2\n",
    "        and s'''\n",
    "   \n",
    "third_part_3 = ''' and b.sentence_id = p.sentence_id) third\n",
    "    from \n",
    "    tokens t\n",
    "    join main on s.token_id = main.token_id\n",
    "    join sentences p on p.sentence_id = main.sentence_id\n",
    "    where t'''\n",
    "\n",
    "fourth_part_3 = ''')\n",
    "    where not (second is null and third is null)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_item_maker(item, num):\n",
    "    for_item = ''\n",
    "    if '\"' in item:\n",
    "        exact_form = re.search('[а-яА-ЯёЁ]+', item)[0]\n",
    "        for_item = '.word = ' + exact_form\n",
    "\n",
    "    elif item in poss:\n",
    "        for_item = '.pos = ' + item\n",
    "\n",
    "    elif '+' in item:\n",
    "        word = re.search('[а-яА-ЯёЁ]+', item)[0]\n",
    "        pos = re.search('[A-Z]+', item)[0]\n",
    "        doc = Doc(word)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemma = token.lemma\n",
    "            if num == 0:\n",
    "                for_item = '.lemma = ' + lemma + ' and t.pos = ' + pos\n",
    "            else:\n",
    "                for_item = '.lemma = ' + lemma + ' and s.pos = ' + pos\n",
    "\n",
    "    else:\n",
    "        doc = Doc(item)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(morph_vocab)\n",
    "            lemma = token.lemma\n",
    "            for_item = '.lemma = ' + lemma\n",
    "    return for_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compilator(request_items=request_items):\n",
    "    for_items = []\n",
    "    if len(request_items) == 2:\n",
    "        for num, item in enumerate(request_items):\n",
    "            for_item = for_item_maker(item, num)\n",
    "            for_items.append(for_item)\n",
    "        query = first_part_2 + for_items[0] + second_part_2 + for_items[1] + third_part_2\n",
    "    \n",
    "    elif len(request_items) == 3:\n",
    "        for num, item in enumerate(request_items):\n",
    "            for_item = for_item_maker(item, num)\n",
    "            for_items.append(for_item)\n",
    "        query = first_part_3 + for_items[0] + second_part_2 + for_items[1] + third_part_3 + for_items[2] + fourth_part_3\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    select * FROM\n",
      "    (select t.token_id, t.word,\n",
      "        (select s.word\n",
      "        from tokens s\n",
      "        join main on s.token_id = main.token_id\n",
      "        join sentences p on p.sentence_id = main.sentence_id\n",
      "        where s.token_id = t.token_id + 1\n",
      "        and s.word = Гарри and b.sentence_id = p.sentence_id) second\n",
      "    from \n",
      "    tokens t\n",
      "    join main on t.token_id = main.token_id\n",
      "    join sentences b on b.sentence_id = main.sentence_id\n",
      "    join texts on b.text_id = texts.text_id\n",
      "    where t.lemma = поттер and b.sentence_id = p.sentence_id) third\n",
      "    from \n",
      "    tokens t\n",
      "    join main on s.token_id = main.token_id\n",
      "    join sentences p on p.sentence_id = main.sentence_id\n",
      "    where t.pos = VERB)\n",
      "    where not (second is null and third is null)\n"
     ]
    }
   ],
   "source": [
    "print(compilator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
